{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f69509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         1  2021-01-01 00:30:10   2021-01-01 00:36:12                1   \n",
      "1         1  2021-01-01 00:51:20   2021-01-01 00:52:19                1   \n",
      "2         1  2021-01-01 00:43:30   2021-01-01 01:11:06                1   \n",
      "3         1  2021-01-01 00:15:48   2021-01-01 00:31:01                0   \n",
      "4         2  2021-01-01 00:31:49   2021-01-01 00:48:21                1   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           2.10           1                  N           142            43   \n",
      "1           0.20           1                  N           238           151   \n",
      "2          14.70           1                  N           132           165   \n",
      "3          10.60           1                  N           138           132   \n",
      "4           4.94           1                  N            68            33   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2          8.0    3.0      0.5        0.00           0.0   \n",
      "1             2          3.0    0.5      0.5        0.00           0.0   \n",
      "2             1         42.0    0.5      0.5        8.65           0.0   \n",
      "3             1         29.0    0.5      0.5        6.05           0.0   \n",
      "4             1         16.5    0.5      0.5        4.06           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  \n",
      "0                    0.3         11.80                   2.5  \n",
      "1                    0.3          4.30                   0.0  \n",
      "2                    0.3         51.95                   0.0  \n",
      "3                    0.3         36.35                   0.0  \n",
      "4                    0.3         24.36                   2.5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# URL for taxi data\n",
    "url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n",
    "\n",
    "# Extract file name from URL\n",
    "\n",
    "csv_name = url.split(\"/\")[-1]\n",
    "\n",
    "# Read compressed CSV directly into a dataframe\n",
    "\n",
    "df = pd.read_csv(csv_name, nrows=100)\n",
    "\n",
    "print(df.head()) #print first 5 rows of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc2289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"yellow_taxi_data\" (\n",
      "\"VendorID\" REAL,\n",
      "  \"tpep_pickup_datetime\" TEXT,\n",
      "  \"tpep_dropoff_datetime\" TEXT,\n",
      "  \"passenger_count\" REAL,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"RatecodeID\" REAL,\n",
      "  \"store_and_fwd_flag\" REAL,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"payment_type\" REAL,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"congestion_surcharge\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#sql.get_schema - generates a CREATE TABLE statement based on structure pandas dataframe.  \n",
    "#This is NOT creating the table in your dbâ€” just generating the SQL string that would do it.\n",
    "#translation: generate the sql string that will create a table from this DataFrame. Call the hypothetical table: 'yellow_taxi_data'\n",
    "\n",
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45fdcee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"yellow_taxi_data\" (\n",
      "\"VendorID\" REAL,\n",
      "  \"tpep_pickup_datetime\" TIMESTAMP,\n",
      "  \"tpep_dropoff_datetime\" TIMESTAMP,\n",
      "  \"passenger_count\" REAL,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"RatecodeID\" REAL,\n",
      "  \"store_and_fwd_flag\" REAL,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"payment_type\" REAL,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"congestion_surcharge\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Convert pickup and dropoff columns from text to datetime objects\n",
    "\n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "# Print updated SQL schema with the new datetime columns\n",
    "\n",
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e27e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" FLOAT(53), \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\tstore_and_fwd_flag FLOAT(53), \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type FLOAT(53), \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tell Pandas: Generate the SQL CREATE TABLE statement for this DataFrame formatted specifically for PostgreSQL.\n",
    "\n",
    "#set up the connection object \n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/nyc_taxi')\n",
    "\n",
    "# Generate the (CREATE TABLE...) statement to send to postgres specifically \n",
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bff004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Tell Pandas - Create the dataframe by reading the csv but don't load whole thing at once. \n",
    "# Read it in batches of 100k rows \n",
    "df_iter = pd.read_csv(csv_name, iterator= True, chunksize=100000)\n",
    "\n",
    "#Call next(df_iter) to get the first chunk of the iterator \n",
    "df = next(df_iter)\n",
    "\n",
    "#print how many rows are in this chunk (length of the iterator)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a61c2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the schema only (no data yet) by using .head(0) to select first 0 rows\n",
    "df.head(0)\n",
    "\n",
    "# Call to_sql to create table in Postgres; if_exists='replace' drops the table if it already exists and creates a fresh one\n",
    "\n",
    "df.head(0).to_sql(name= 'yellow_taxi_data', con= engine, if_exists= 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d049bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start to load data into the table \n",
    "df.to_sql(name= 'yellow_taxi_data', con= engine, if_exists= 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a0e5b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows found: 0\n"
     ]
    }
   ],
   "source": [
    "#Check if any duplicates since we just ran the last cell twice. \n",
    "dupes_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows found: {dupes_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3b5df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested chunk 1, took 3.687seconds\n",
      "Ingested chunk 2, took 3.741seconds\n",
      "Ingested chunk 3, took 3.655seconds\n",
      "Ingested chunk 4, took 3.631seconds\n",
      "Ingested chunk 5, took 3.698seconds\n",
      "Ingested chunk 6, took 3.592seconds\n",
      "Ingested chunk 7, took 3.625seconds\n",
      "Ingested chunk 8, took 3.652seconds\n",
      "Ingested chunk 9, took 3.820seconds\n",
      "Ingested chunk 10, took 3.704seconds\n",
      "Ingested chunk 11, took 3.614seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/whjpvhfd7yg_pntm9383gb8c0000gn/T/ipykernel_15282/3852880683.py:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, df in enumerate(df_iter):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested chunk 12, took 3.637seconds\n",
      "Ingested chunk 13, took 2.504seconds\n"
     ]
    }
   ],
   "source": [
    "#Continue ingesting data in batches using for loop \n",
    "#time each ingestion and print how look it took\n",
    "\n",
    "from time import time \n",
    "\n",
    "for i, df in enumerate(df_iter): \n",
    "    \n",
    "    start_t = time()\n",
    "\n",
    "    df.to_sql(name='yellow_taxi_data', con=engine, if_exists= 'append')\n",
    "\n",
    "    end_t = time()\n",
    "    print(f'Ingested chunk {i +1}, took {end_t - start_t:.3f}seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
